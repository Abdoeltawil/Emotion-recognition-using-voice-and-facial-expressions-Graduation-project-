{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  <center> Speech Emotion Recognition <center>","metadata":{}},{"cell_type":"code","source":"!pip install librosa\n! apt-get update\n! apt-get install -y libsndfile-dev ","metadata":{"execution":{"iopub.status.busy":"2022-07-03T17:51:57.071058Z","iopub.execute_input":"2022-07-03T17:51:57.072206Z","iopub.status.idle":"2022-07-03T17:52:25.238668Z","shell.execute_reply.started":"2022-07-03T17:51:57.072051Z","shell.execute_reply":"2022-07-03T17:52:25.237314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-07-03T17:52:25.241692Z","iopub.execute_input":"2022-07-03T17:52:25.242666Z","iopub.status.idle":"2022-07-03T17:52:38.899605Z","shell.execute_reply.started":"2022-07-03T17:52:25.242604Z","shell.execute_reply":"2022-07-03T17:52:38.897492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation\n* As we are working with four different datasets, so i will be creating a dataframe storing all emotions of the data in dataframe with their paths.\n* We will use this dataframe to extract features for our model training.","metadata":{}},{"cell_type":"code","source":"# Paths for data.\n# Ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n# Crema = \"/kaggle/input/cremad/AudioWAV/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n# Savee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"","metadata":{"execution":{"iopub.status.busy":"2022-05-16T02:36:55.789527Z","iopub.execute_input":"2022-05-16T02:36:55.78986Z","iopub.status.idle":"2022-05-16T02:36:55.794675Z","shell.execute_reply.started":"2022-05-16T02:36:55.789824Z","shell.execute_reply":"2022-05-16T02:36:55.793734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  <center> 3. TESS dataset <center>","metadata":{}},{"cell_type":"code","source":"tess_directory_list = os.listdir(Tess)\n\nfile_emotion = []\nfile_path = []\n\nfor dir in tess_directory_list:\n    directories = os.listdir(Tess + dir)\n    for file in directories:\n        part = file.split('.')[0]\n        part = part.split('_')[2]\n        if part=='ps':\n            file_emotion.append('surprise')\n        else:\n            file_emotion.append(part)\n        file_path.append(Tess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nTess_df = pd.concat([emotion_df, path_df], axis=1)\nTess_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T02:36:55.79604Z","iopub.execute_input":"2022-05-16T02:36:55.796632Z","iopub.status.idle":"2022-05-16T02:36:56.814395Z","shell.execute_reply.started":"2022-05-16T02:36:55.79659Z","shell.execute_reply":"2022-05-16T02:36:56.813464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating Dataframe using all the 4 dataframes we created so far.\ndata_path = pd.concat([ Tess_df], axis = 0)\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T02:36:56.816502Z","iopub.execute_input":"2022-05-16T02:36:56.816886Z","iopub.status.idle":"2022-05-16T02:36:57.002458Z","shell.execute_reply.started":"2022-05-16T02:36:56.816847Z","shell.execute_reply":"2022-05-16T02:36:57.001482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualisation and Exploration","metadata":{}},{"cell_type":"markdown","source":"First let's plot the count of each emotions in our dataset.","metadata":{}},{"cell_type":"code","source":"plt.title('Count of Emotions', size=16)\nsns.countplot(data_path.Emotions)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T02:37:04.940939Z","iopub.execute_input":"2022-05-16T02:37:04.941291Z","iopub.status.idle":"2022-05-16T02:37:05.145387Z","shell.execute_reply.started":"2022-05-16T02:37:04.941259Z","shell.execute_reply":"2022-05-16T02:37:05.144601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also plot waveplots and spectograms for audio signals\n\n* Waveplots - Waveplots let us know the loudness of the audio at a given time.\n* Spectograms - A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. Itâ€™s a representation of frequencies changing with respect to time for given audio/music signals.","metadata":{}},{"cell_type":"code","source":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n    librosa.display.waveplot(data, sr=sr)\n    plt.show()\n\ndef create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   \n    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T02:37:06.757314Z","iopub.execute_input":"2022-05-16T02:37:06.757697Z","iopub.status.idle":"2022-05-16T02:37:06.766071Z","shell.execute_reply.started":"2022-05-16T02:37:06.757664Z","shell.execute_reply":"2022-05-16T02:37:06.765014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='fear'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:43:14.544878Z","iopub.execute_input":"2022-02-15T11:43:14.545222Z","iopub.status.idle":"2022-02-15T11:43:15.637031Z","shell.execute_reply.started":"2022-02-15T11:43:14.54519Z","shell.execute_reply":"2022-02-15T11:43:15.635532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='angry'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:43:36.91178Z","iopub.execute_input":"2022-02-15T11:43:36.912133Z","iopub.status.idle":"2022-02-15T11:43:37.337345Z","shell.execute_reply.started":"2022-02-15T11:43:36.912101Z","shell.execute_reply":"2022-02-15T11:43:37.336643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='sad'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:43:46.174823Z","iopub.execute_input":"2022-02-15T11:43:46.175187Z","iopub.status.idle":"2022-02-15T11:43:46.627507Z","shell.execute_reply.started":"2022-02-15T11:43:46.175156Z","shell.execute_reply":"2022-02-15T11:43:46.626669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='happy'\npath = np.array(data_path.Path[data_path.Emotions==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:44:01.649422Z","iopub.execute_input":"2022-02-15T11:44:01.649792Z","iopub.status.idle":"2022-02-15T11:44:02.094534Z","shell.execute_reply.started":"2022-02-15T11:44:01.649741Z","shell.execute_reply":"2022-02-15T11:44:02.093673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation\n\n- Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set.\n- To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed.\n- The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\n- In order to this to work adding the perturbations must conserve the same label as the original training sample.\n- In images data augmention can be performed by shifting the image, zooming, rotating ...\n\nFirst, let's check which augmentation techniques works better for our dataset.","metadata":{}},{"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n\n# taking any example and checking for techniques.\ndata, sample_rate = librosa.load('../input/arabicemotions/wav/angrywav/1-04 Sorcerian.wav')","metadata":{"execution":{"iopub.status.busy":"2022-07-03T18:02:17.442318Z","iopub.execute_input":"2022-07-03T18:02:17.442732Z","iopub.status.idle":"2022-07-03T18:02:25.275302Z","shell.execute_reply.started":"2022-07-03T18:02:17.442699Z","shell.execute_reply":"2022-07-03T18:02:25.273828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Simple Audio","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=data, sr=sample_rate)\nAudio(path)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:44:16.641353Z","iopub.execute_input":"2022-02-15T11:44:16.641708Z","iopub.status.idle":"2022-02-15T11:44:16.859531Z","shell.execute_reply.started":"2022-02-15T11:44:16.641675Z","shell.execute_reply":"2022-02-15T11:44:16.858641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Noise Injection","metadata":{}},{"cell_type":"code","source":"x = noise(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:44:22.296724Z","iopub.execute_input":"2022-02-15T11:44:22.297115Z","iopub.status.idle":"2022-02-15T11:44:22.511649Z","shell.execute_reply.started":"2022-02-15T11:44:22.297082Z","shell.execute_reply":"2022-02-15T11:44:22.510726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see noise injection is a very good augmentation technique because of which we can assure our training model is not overfitted","metadata":{}},{"cell_type":"markdown","source":"#### 3. Stretching","metadata":{}},{"cell_type":"code","source":"x = stretch(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:44:28.795222Z","iopub.execute_input":"2022-02-15T11:44:28.795559Z","iopub.status.idle":"2022-02-15T11:44:29.531972Z","shell.execute_reply.started":"2022-02-15T11:44:28.795528Z","shell.execute_reply":"2022-02-15T11:44:29.530898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Shifting","metadata":{}},{"cell_type":"code","source":"x = shift(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:44:36.14619Z","iopub.execute_input":"2022-02-15T11:44:36.146542Z","iopub.status.idle":"2022-02-15T11:44:36.363077Z","shell.execute_reply.started":"2022-02-15T11:44:36.146509Z","shell.execute_reply":"2022-02-15T11:44:36.362021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Pitch","metadata":{}},{"cell_type":"code","source":"x = pitch(data, sample_rate)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:44:40.800522Z","iopub.execute_input":"2022-02-15T11:44:40.800954Z","iopub.status.idle":"2022-02-15T11:44:41.041731Z","shell.execute_reply.started":"2022-02-15T11:44:40.800909Z","shell.execute_reply":"2022-02-15T11:44:41.040855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above types of augmentation techniques i am using noise, stretching(ie. changing speed) and some pitching.","metadata":{}},{"cell_type":"code","source":"def extract_features(data):\n    # ZCR\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result=np.hstack((result, zcr)) # stacking horizontally\n\n    # Chroma_stft\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, chroma_stft)) # stacking horizontally\n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mfcc)) # stacking horizontally\n\n    # Root Mean Square Value\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    result = np.hstack((result, rms)) # stacking horizontally\n\n    # MelSpectogram\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mel)) # stacking horizontally\n    \n    return result\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n    # without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    \n    # data with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    result = np.vstack((result, res2)) # stacking vertically\n    \n    # data with stretching and pitching\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sample_rate)\n    res3 = extract_features(data_stretch_pitch)\n    result = np.vstack((result, res3)) # stacking vertically\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2022-07-03T18:03:19.751469Z","iopub.execute_input":"2022-07-03T18:03:19.751990Z","iopub.status.idle":"2022-07-03T18:03:19.771053Z","shell.execute_reply.started":"2022-07-03T18:03:19.751953Z","shell.execute_reply":"2022-07-03T18:03:19.769427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nsad = glob.glob('../input/arabicemotions/wav/sadwav/*')\nangry = glob.glob('../input/arabicemotions/wav/angrywav/*')\nneutral = glob.glob('../input/arabicemotions/wav/neutralwav/*')","metadata":{"execution":{"iopub.status.busy":"2022-07-03T17:56:38.523420Z","iopub.execute_input":"2022-07-03T17:56:38.523872Z","iopub.status.idle":"2022-07-03T17:56:38.602262Z","shell.execute_reply.started":"2022-07-03T17:56:38.523840Z","shell.execute_reply":"2022-07-03T17:56:38.601335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.shuffle(neutral)\nneutral=neutral[0:150]","metadata":{"execution":{"iopub.status.busy":"2022-07-03T17:58:25.112298Z","iopub.execute_input":"2022-07-03T17:58:25.112690Z","iopub.status.idle":"2022-07-03T17:58:25.119072Z","shell.execute_reply.started":"2022-07-03T17:58:25.112659Z","shell.execute_reply":"2022-07-03T17:58:25.117395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, Y = [], []\nfor path in(sad):\n    feature = get_features(path)\n    for ele in feature:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append('sad')\n\nfor path in(angry):\n    feature = get_features(path)\n    for ele in feature:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append('angry')\n        \nfor path in(neutral):\n    feature = get_features(path)\n    for ele in feature:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append('neutral')","metadata":{"execution":{"iopub.status.busy":"2022-07-03T18:03:22.987865Z","iopub.execute_input":"2022-07-03T18:03:22.989388Z","iopub.status.idle":"2022-07-03T18:05:22.559085Z","shell.execute_reply.started":"2022-07-03T18:03:22.989317Z","shell.execute_reply":"2022-07-03T18:05:22.557102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, Y = [], []\nfor path, emotion in zip(data_path.Path, data_path.Emotions):\n    feature = get_features(path)\n    for ele in feature:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T02:37:20.137417Z","iopub.execute_input":"2022-05-16T02:37:20.137745Z","iopub.status.idle":"2022-05-16T02:46:30.269159Z","shell.execute_reply.started":"2022-05-16T02:37:20.137716Z","shell.execute_reply":"2022-05-16T02:46:30.268126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-03T18:13:43.424286Z","iopub.execute_input":"2022-07-03T18:13:43.424711Z","iopub.status.idle":"2022-07-03T18:13:43.435134Z","shell.execute_reply.started":"2022-07-03T18:13:43.424677Z","shell.execute_reply":"2022-07-03T18:13:43.433647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X), len(Y), data_path.Path.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-16T02:46:30.270827Z","iopub.execute_input":"2022-05-16T02:46:30.271146Z","iopub.status.idle":"2022-05-16T02:46:30.282277Z","shell.execute_reply.started":"2022-05-16T02:46:30.271109Z","shell.execute_reply":"2022-05-16T02:46:30.28144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.to_csv('features.csv', index=False)\nFeatures.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T03:14:28.791971Z","iopub.execute_input":"2022-05-16T03:14:28.792292Z","iopub.status.idle":"2022-05-16T03:14:32.883255Z","shell.execute_reply.started":"2022-05-16T03:14:28.792262Z","shell.execute_reply":"2022-05-16T03:14:32.882149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We have applied data augmentation and extracted the features for each audio files and saved them.","metadata":{}},{"cell_type":"markdown","source":"## Data Preparation\n\n- As of now we have extracted the data, now we need to normalize and split our data for training and testing.","metadata":{}},{"cell_type":"code","source":"!zip -r file.zip \"./\"","metadata":{"execution":{"iopub.status.busy":"2022-05-16T03:18:08.554964Z","iopub.execute_input":"2022-05-16T03:18:08.555298Z","iopub.status.idle":"2022-05-16T03:18:12.005659Z","shell.execute_reply.started":"2022-05-16T03:18:08.555266Z","shell.execute_reply":"2022-05-16T03:18:12.004782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'./')\nfrom IPython.display import FileLink\nFileLink(r'features.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-16T03:20:23.105526Z","iopub.execute_input":"2022-05-16T03:20:23.105875Z","iopub.status.idle":"2022-05-16T03:20:23.11255Z","shell.execute_reply.started":"2022-05-16T03:20:23.105844Z","shell.execute_reply":"2022-05-16T03:20:23.111481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import stat\nimport os\npath = 'file.zip'\nst = os.stat(path)\nos.chmod(path, stat.S_IXUSR)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T03:19:03.610389Z","iopub.execute_input":"2022-05-16T03:19:03.610755Z","iopub.status.idle":"2022-05-16T03:19:03.617464Z","shell.execute_reply.started":"2022-05-16T03:19:03.610723Z","shell.execute_reply":"2022-05-16T03:19:03.616434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"features.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T02:54:06.615913Z","iopub.execute_input":"2022-05-16T02:54:06.616243Z","iopub.status.idle":"2022-05-16T02:54:06.992907Z","shell.execute_reply.started":"2022-05-16T02:54:06.616207Z","shell.execute_reply":"2022-05-16T02:54:06.991966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:54:30.426894Z","iopub.execute_input":"2022-02-15T11:54:30.427241Z","iopub.status.idle":"2022-02-15T11:54:30.440444Z","shell.execute_reply.started":"2022-02-15T11:54:30.42721Z","shell.execute_reply":"2022-02-15T11:54:30.439347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T11:54:32.952132Z","iopub.execute_input":"2022-02-15T11:54:32.952464Z","iopub.status.idle":"2022-02-15T11:54:32.963691Z","shell.execute_reply.started":"2022-02-15T11:54:32.952432Z","shell.execute_reply":"2022-02-15T11:54:32.962842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:13:32.062894Z","iopub.execute_input":"2022-02-15T12:13:32.063266Z","iopub.status.idle":"2022-02-15T12:13:32.080245Z","shell.execute_reply.started":"2022-02-15T12:13:32.063232Z","shell.execute_reply":"2022-02-15T12:13:32.07915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:13:34.337539Z","iopub.execute_input":"2022-02-15T12:13:34.337887Z","iopub.status.idle":"2022-02-15T12:13:34.36038Z","shell.execute_reply.started":"2022-02-15T12:13:34.337851Z","shell.execute_reply":"2022-02-15T12:13:34.359485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making our data compatible to model.\nx_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:13:37.280795Z","iopub.execute_input":"2022-02-15T12:13:37.281125Z","iopub.status.idle":"2022-02-15T12:13:37.287726Z","shell.execute_reply.started":"2022-02-15T12:13:37.281096Z","shell.execute_reply":"2022-02-15T12:13:37.286849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:15:26.401855Z","iopub.execute_input":"2022-02-15T12:15:26.402282Z","iopub.status.idle":"2022-02-15T12:15:26.408663Z","shell.execute_reply.started":"2022-02-15T12:15:26.402246Z","shell.execute_reply":"2022-02-15T12:15:26.407733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"model=Sequential()\n\n# model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\n# model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\n# model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n# model.add(Dropout(0.2))\n\n# model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(keras.layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\nmodel.add(keras.layers.LSTM(128))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=7, activation='softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:20:46.876779Z","iopub.execute_input":"2022-02-15T12:20:46.877116Z","iopub.status.idle":"2022-02-15T12:20:47.101992Z","shell.execute_reply.started":"2022-02-15T12:20:46.877088Z","shell.execute_reply":"2022-02-15T12:20:47.10127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\nhistory=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:21:03.072247Z","iopub.execute_input":"2022-02-15T12:21:03.072568Z","iopub.status.idle":"2022-02-15T12:44:23.582726Z","shell.execute_reply.started":"2022-02-15T12:21:03.072539Z","shell.execute_reply":"2022-02-15T12:44:23.582012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:44:39.138224Z","iopub.execute_input":"2022-02-15T12:44:39.138662Z","iopub.status.idle":"2022-02-15T12:44:42.239579Z","shell.execute_reply.started":"2022-02-15T12:44:39.138621Z","shell.execute_reply":"2022-02-15T12:44:42.238733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:44:50.595924Z","iopub.execute_input":"2022-02-15T12:44:50.596246Z","iopub.status.idle":"2022-02-15T12:44:53.383531Z","shell.execute_reply.started":"2022-02-15T12:44:50.596219Z","shell.execute_reply":"2022-02-15T12:44:53.382686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:45:02.259578Z","iopub.execute_input":"2022-02-15T12:45:02.260044Z","iopub.status.idle":"2022-02-15T12:45:02.279651Z","shell.execute_reply.started":"2022-02-15T12:45:02.26001Z","shell.execute_reply":"2022-02-15T12:45:02.278853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:45:10.595891Z","iopub.execute_input":"2022-02-15T12:45:10.596216Z","iopub.status.idle":"2022-02-15T12:45:10.952697Z","shell.execute_reply.started":"2022-02-15T12:45:10.596184Z","shell.execute_reply":"2022-02-15T12:45:10.951751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:45:15.824254Z","iopub.execute_input":"2022-02-15T12:45:15.824617Z","iopub.status.idle":"2022-02-15T12:45:15.907778Z","shell.execute_reply.started":"2022-02-15T12:45:15.824582Z","shell.execute_reply":"2022-02-15T12:45:15.906718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see our model is more accurate in predicting surprise, angry emotions and it makes sense also because audio files of these emotions differ to other audio files in a lot of ways like pitch, speed etc..\n- We overall achieved 61% accuracy on our test data and its decent but we can improve it more by applying more augmentation techniques and using other feature extraction methods.","metadata":{}},{"cell_type":"code","source":"model.save(\"sound_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-02-15T12:46:46.954427Z","iopub.execute_input":"2022-02-15T12:46:46.954786Z","iopub.status.idle":"2022-02-15T12:46:47.000752Z","shell.execute_reply.started":"2022-02-15T12:46:46.954736Z","shell.execute_reply":"2022-02-15T12:46:47.000064Z"},"trusted":true},"execution_count":null,"outputs":[]}]}